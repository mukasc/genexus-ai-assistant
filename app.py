import os
import streamlit as st
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# Carrega a API Key do arquivo .env
load_dotenv("keys.env")

API_KEY = os.getenv("GEMINI_API_KEY")

# --- Configura√ß√µes Iniciais ---
st.set_page_config(page_title="GeneXus AI Assistant (RAG)", layout="wide")
st.title("ü§ñ GeneXus AI Assistant (Prot√≥tipo RAG)")
st.caption("Especialista em GeneXus alimentado pela documenta√ß√£o oficial e Gemini API.")

# O Prompt Template √© a otimiza√ß√£o crucial para especializar o LLM
PROMPT_TEMPLATE_OLD = """
Voc√™ √© um assistente de programa√ß√£o **perito em GeneXus**. Sua fun√ß√£o √© auxiliar o desenvolvedor a escrever c√≥digo, modelar objetos e entender conceitos GeneXus.
**Seu foco deve ser na sintaxe e nos objetos GeneXus (Transactions, Data Providers, Procedures, Web Panels) e n√£o em linguagens de programa√ß√£o subjacentes (Java, C#, etc.).**

**INSTRU√á√ïES:**
1.  Use **APENAS** as informa√ß√µes contidas no 'CONTEXTO' abaixo para formular sua resposta.
2.  Responda de forma clara e t√©cnica.
3.  Quando gerar c√≥digo GeneXus, use blocos de c√≥digo (` ``` `) e especifique o tipo (ex: ` ```genexus` ou ` ```sql`).
4.  Se o contexto n√£o for suficiente, diga educadamente que, com a sua base de conhecimento atual, voc√™ n√£o pode responder √† pergunta espec√≠fica sobre GeneXus.
5.  Mantenha a resposta focada no tema GeneXus.

CONTEXTO:
{context}

PERGUNTA DO USU√ÅRIO: {question}
"""

PROMPT_TEMPLATE_OTIMIZED = """
Voc√™ √© o **GeneXus Code Assistant**, um especialista s√™nior em GeneXus (todas as vers√µes) e engenharia de software Low-Code.
Sua miss√£o √© fornecer solu√ß√µes completas, robustas e que sigam as **melhores pr√°ticas de modelagem e programa√ß√£o GeneXus**.

**DIRETRIZES DE C√ìDIGO E RESPOSTA:**
1.  **Prioridade GeneXus:** Sempre que a pergunta for sobre implementa√ß√£o ou sintaxe, priorize a cria√ß√£o de c√≥digo **EXCLUSIVAMENTE em sintaxe GeneXus**.
2.  **Formato:** O c√≥digo GeneXus deve ser envolto em blocos de c√≥digo (` ```genexus`) para clareza. Para regras SQL/Data Selectors, use (` ```sql`).
3.  **Melhores Pr√°ticas:** Se o contexto recuperado mencionar otimiza√ß√µes (ex: uso de For Each com condi√ß√µes *inferred*, minimiza√ß√£o de acessos a banco de dados), **integre-as** na sua sugest√£o de c√≥digo.
4.  **Estrita Fidelidade ao Contexto (RAG):** Sua resposta deve ser **inteiramente baseada no 'CONTEXTO'** fornecido. N√£o invente ou combine informa√ß√µes de conhecimento geral.
5.  **Rejei√ß√£o Inteligente:** Se o contexto for insuficiente ou irrelevante, recuse-se a responder, informando que a base de conhecimento (documenta√ß√£o) n√£o cobre o t√≥pico.
6.  **Foco em Objeto:** Para requisi√ß√µes de modelagem (ex: 'criar um Data Provider'), entregue o c√≥digo completo da estrutura do objeto.

CONTEXTO (Documenta√ß√£o GeneXus e Tutoriais):
{context}

PERGUNTA DO USU√ÅRIO: {question}
"""

PROMPT_TEMPLATE = """
Voc√™ √© o **GeneXus Code Assistant**, um especialista s√™nior em GeneXus. Sua miss√£o √© fornecer solu√ß√µes completas e robustas, seguindo as melhores pr√°ticas.

**DIRETRIZES DE C√ìDIGO E RESPOSTA:**
1.  **Prioridade GeneXus:** Sempre gere c√≥digo **EXCLUSIVAMENTE em sintaxe GeneXus**. Use blocos de c√≥digo (` ```genexus`).
2.  **Foco em Dados Estruturados:** Priorize informa√ß√µes encontradas em **tabelas, listas de propriedades e defini√ß√µes de sintaxe** dentro do 'CONTEXTO'. Estes dados textuais s√£o a sua fonte de verdade, compensando a aus√™ncia de diagramas visuais.
3.  **Infer√™ncia Contextual:** Se o 'CONTEXTO' descrever um processo ou fluxo de dados (que pode ter sido originalmente um diagrama), **infira o fluxo l√≥gico** e traduza-o para a sintaxe GeneXus correta (ex: *par√¢metros, comandos de Procedure*).
4.  **Estrita Fidelidade ao Contexto (RAG):** Sua resposta deve ser **inteiramente baseada no 'CONTEXTO'** fornecido.
5.  **Rejei√ß√£o Inteligente:** Se o contexto for insuficiente, recuse-se a responder.
6.  **Idioma: Deve interpretar todos os idiomas que conhece mas a resposta deve ser sempre em PT-BR ou no idioma fornecido.

CONTEXTO (Documenta√ß√£o GeneXus e Tutoriais):
{context}

PERGUNTA DO USU√ÅRIO: {question}
"""


@st.cache_resource
def get_retriever():
    """Carrega o banco de dados vetorial e cria o Retriever."""
    # Garante que a API Key esteja dispon√≠vel
    if not API_KEY:
        st.error("A vari√°vel de ambiente GEMINI_API_KEY n√£o est√° configurada.")
        st.stop()
        
    embeddings = GoogleGenerativeAIEmbeddings(
        model="text-embedding-004",
        google_api_key=API_KEY
        )
    
    # Conecta ao Vector Store persistido
    try:
        vectorstore = Chroma(
            persist_directory="./chroma_db",
            embedding_function=embeddings
        )
        # k=3: busca os 3 chunks mais relevantes
        return vectorstore.as_retriever(search_kwargs={"k": 3}) 
    except Exception as e:
        st.error(f"Erro ao carregar o banco de dados. Execute 'python ingest.py'. Erro: {e}")
        st.stop()

# 1. Obter o Retriever
retriever = get_retriever()

# 2. Configurar o LLM (Gemini)
llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.1)

# 3. Criar a Cadeia RAG (LangChain Expression Language - LCEL)
prompt = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)

def format_docs(docs):
    """Formata os documentos recuperados em uma string simples."""
    return "\n\n".join(doc.page_content for doc in docs)

# O pipe RAG: Contexto -> Prompt -> LLM -> Resposta
rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

# --- Interface Streamlit ---

if "messages" not in st.session_state:
    st.session_state.messages = []

# Exibir hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Capturar nova entrada do usu√°rio
if prompt_input := st.chat_input("Pergunte algo sobre GeneXus..."):
    st.session_state.messages.append({"role": "user", "content": prompt_input})
    with st.chat_message("user"):
        st.markdown(prompt_input)

    # Gerar resposta da IA
    with st.chat_message("assistant"):
        with st.spinner("Pensando como um especialista GeneXus..."):
            response = rag_chain.invoke(prompt_input)
            st.markdown(response)
            
    st.session_state.messages.append({"role": "assistant", "content": response})

# Sidebar para informa√ß√µes adicionais
st.sidebar.header("Status do Prot√≥tipo")
st.sidebar.markdown(f"**Framework RAG:** LangChain")
st.sidebar.markdown(f"**LLM:** Gemini 2.5 Flash")
st.sidebar.markdown(f"**Vector Store:** ChromaDB")
